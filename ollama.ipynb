{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Download and install ollama to the system\n!curl https://ollama.ai/install.sh | sh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0djekjfijO8L","outputId":"97459cfd-ccde-45dc-bdb2-1906499ffb40","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install aiohttp pyngrok nest_asyncio\n\nimport os\nimport asyncio\nimport subprocess\n\n# 1) Keep your LD_LIBRARY_PATH tweak\nos.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia' + os.pathsep + os.environ.get('LD_LIBRARY_PATH','')\n\n# 2) Detect all NVIDIA GPUs and export CUDA_VISIBLE_DEVICES\ndef detect_and_set_gpus():\n    try:\n        out = subprocess.check_output(\n            ['nvidia-smi','--query-gpu=index','--format=csv,noheader'],\n            stderr=subprocess.DEVNULL,\n            encoding='utf-8'\n        )\n        gpus = [line.strip() for line in out.splitlines() if line.strip().isdigit()]\n        if gpus:\n            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpus)\n            print(f\"✅ CUDA_VISIBLE_DEVICES set to {os.environ['CUDA_VISIBLE_DEVICES']}\")\n        else:\n            print(\"⚠️  No GPUs found, running CPU only.\")\n    except Exception as e:\n        print(\"⚠️  GPU detection error:\", e)\n\ndetect_and_set_gpus()\n\nasync def run_process(cmd):\n    print('>>> starting', *cmd)\n    p = await asyncio.subprocess.create_subprocess_exec(\n        *cmd,\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n    )\n    async def pipe(stream):\n        async for line in stream:\n            print(line.decode('utf-8').rstrip())\n    await asyncio.gather(pipe(p.stdout), pipe(p.stderr))\n\nasync def main():\n    # your ngrok auth\n    await run_process(['ngrok', 'config', 'add-authtoken', '1wyabPDW6B1CJdgEafS9x7BhK14_586mRJv9Je3QhJ2Bw4ucV'])\n    \n    # ollama serve – it’ll now see the GPUs via CUDA_VISIBLE_DEVICES\n    await asyncio.gather(\n        run_process(['ollama', 'serve']),\n        run_process([\n            'ngrok', 'http',\n            '--log', 'stderr',\n            '11434',\n            '--host-header', 'localhost:11434'\n        ]),\n    )\n\n# 3) Robust entrypoint: use asyncio.run or fallback to existing loop\nif __name__ == '__main__':\n    try:\n        asyncio.run(main())\n    except RuntimeError as e:\n        # likely \"loop is running\" — install and apply nest_asyncio\n        import nest_asyncio\n        nest_asyncio.apply()\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(main())\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YWFb81i3jh4A","outputId":"6de69e46-d56b-4480-8171-91197cef1ce2","trusted":true},"outputs":[],"execution_count":null}]}